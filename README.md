## Genetic Algorithms and Differential Evolution
*Methods for Solving Constrained Single-Objective Optimization Problems*

> This project explores the use of Genetic Algorithms (GA) and Differential Evolution (DE) for solving constrained single-objective optimization problems. The implementation includes custom configurations, statistical analysis, and visualization tools to enhance the optimization process.

---

## Table of Contents

- [Genetic Algorithms and Differential Evolution](#genetic-algorithms-and-differential-evolution)
- [Table of Contents](#table-of-contents)
- [Features](#features)
- [Genetic Operators](#genetic-operators)
  - [Selection](#selection)
  - [Crossover](#crossover)
  - [Mutation](#mutation)
- [Implemented Strategies](#implemented-strategies)
  - [Customizing Strategies](#customizing-strategies)
- [Constraint-Handling Technique](#constraint-handling-technique)
  - [Benchmark Problems](#benchmark-problems)
- [Dependencies](#dependencies)
  - [Core Libraries](#core-libraries)
  - [Additional Modules and Files](#additional-modules-and-files)
  - [Project-Specific Modules](#project-specific-modules)
  - [Configuration Parser](#configuration-parser)
  - [Installation](#installation)
- [Use](#use)
  - [Creating the population](#creating-the-population)
  - [Evolving the population](#evolving-the-population)
  - [Retrieving Population Information](#retrieving-population-information)
    - [Get Population Statistics](#get-population-statistics)
    - [Accessing the Best Chromosome's Genes](#accessing-the-best-chromosomes-genes)

---

## Features

- **Genetic Algorithm (GA)** and **Differential Evolution (DE)** implementations for constrained optimization.
- **Customizable configuration** using `configparser`.
- **Statistical Analysis** using rank-sum tests from SciPy.
- **Visualization** tools for results, including data plotting with Matplotlib.
- **Modular code structure** for ease of customization and extension.

---

## Genetic Operators
 
### Selection
- **Tournament Selection**: A selection method that can use either a deterministic or probabilistic approach, controlled by the parameter `p`. Additionally, the parameter `q` controls the number of individuals that participate in each match.
- **Parent vs Child**: Implements strategies like `*/best/y/z` for selecting individuals based on performance and variation criteria in the population.
- **Roulette Wheel Selection**: A probabilistic selection method where the chance of selecting an individual is proportional to its fitness.
- **Stochastic Ranking**: A stochastic selection method used for constrained optimization, which balances between fitness and constraint satisfaction. This trade-off is controlled by the `Pf` parameter.

### Crossover
- **Binomial/Binary Crossover**: A crossover method where offspring are generated by randomly selecting genes from each parent.
- **Simulated Binary Crossover (SBX)**: Mimics the behavior of binary-coded crossover in real-valued search spaces.

### Mutation
- **Parameter-based mutation (PBM)**: Adjusts parameters of individuals within the population by small random changes that depend on the generation number.
- **Differential Evolution**: A mutation strategy specific to the Differential Evolution algorithm, where the mutation is based on the difference between randomly selected individuals.

---
## Implemented Strategies

Not all of the aforementioned operators are used in each strategy. The implemented strategies and their selected operators are as follows:

- **GA**: Uses **Roulette Wheel Selection**, **Simulated Binary Crossover (SBX)**, and **Parameter-Based Mutation (PBM)**.
- **DE**: Follows the **DE/best/1/bin** scheme with **Binomial Crossover** and **Differential Evolution Mutation**.
- **DE+SR**: Similar to **DE/best/1/bin** but incorporates **Stochastic Ranking (SR)** for handling constraints.

### Customizing Strategies

To customize a strategy to your liking, you can modify the parameters and operators within the `evolve` method in the `Population` class. This allows you to adjust selection methods, crossover types, mutation rates, and other genetic operators directly, giving you full control over the optimization process.

The `evolve` method serves as the core of the evolutionary process, where you can fine-tune the behavior of each strategy to better suit specific problem requirements.

---
## Constraint-Handling Technique
Given the minimization objective function $f(x)$ and the equality contraints $h(x)$ and inequality constraints $g(x)$, the problem is now to minimize the penalized objective function $\phi(x)$.

$$ 
\phi(x) = f(x) + \left[ \sum_{i=1}^{p} r_{i,t} h_i(x)^{\alpha_t} + \sum_{i=1}^{p} c_{i,t} g_i(x)^{\beta_t} \right] 
$$

After transforming the equality constraints into inequality constraints with a relaxation term $\delta$

$$
h(x) = c \Rightarrow 
\begin{cases}
    g_1(x) = h(x) - c - \delta \leq 0 \\
    g_2(x) = -h(x) + c + \delta \leq 0
\end{cases}
$$

The final penalized objective function is
$$
\bm{\phi}(x) = \bm{f}(x) + \sum_{i=1}^{p} c_{i,t} g_i(x)^{\beta_t} 
$$

Where $c_{i,t}$, $\beta^t$ and $\delta_t$ depend on the generation number $t$ and are calculated as a linear interpolation between the maximum penalty constant $a_{max}$ and the minimum penalty constant $a_{min}$:

$$
a_t = a_{min} + \frac{t}{T} \left( a_{max} - a_{min} \right)
$$

---
### Benchmark Problems

The implemented problems include classic benchmark functions and challenging problems from key research papers in evolutionary optimization:

- **Classic Functions**:
  - **Sphere Function**: A simple, convex function used to test optimization algorithms' ability to reach the global minimum.
  - **Rosenbrock Function**: Also known as the "valley" or "banana" function, this function has a narrow, curved valley, making it difficult for some algorithms to find the minimum.
  - **Rastrigin Function**: A non-convex function with many local minima, commonly used to test an algorithm's robustness in finding the global optimum in a complex search space.

- **Advanced Benchmark Functions**:
  - Selected problems from:
    - Michalewicz, Z., & Schoenauer, M. (1996). *Evolutionary algorithms for constrained parameter optimization problems*. Evolutionary Computation, 4(1), 1-32.
    - Layeb, A. (2022). *New hard benchmark functions for global optimization*. arXiv preprint arXiv:2202.04606.

---

## Dependencies

This project requires the following Python libraries and custom modules:

### Core Libraries

- **NumPy** (`numpy`): For numerical computations.
- **Pandas** (`pandas`): For data manipulation and analysis.
- **Random** (`random`): For generating random numbers.
- **Matplotlib** (`matplotlib.pyplot`): For plotting and visualizing results.
- **SciPy** (`scipy.stats`): For statistical analysis, specifically the rank-sum test.

### Additional Modules and Files

- **ABC Meta** (`abc`): Provides abstract base class functionality, used to define abstract classes and methods.

### Project-Specific Modules

The following are custom modules included in this project. Ensure these files are in the project directory:

- **Utils** (`utils`): Contains utility functions used throughout the project.
- **Problems** (`Problems`): Defines the problem(s) to be solved by the optimization algorithms.
- **Genetic Algorithm (GA)** (`GA`): Contains the Genetic Algorithm implementation used in this project.

### Configuration Parser

- **ConfigParser** (`configparser`): Used to handle configuration files, making it easier to set parameters and other configurations.

### Installation

To install the main dependencies, run:
```bash
pip install numpy pandas matplotlib scipy
```
---
## Use

### Creating the population
- To initialize a population, specify the instance problem, configuration file, and additional parameters such as the number of individuals, number of generations, penalty factors, and whether the penalty is included in the fitness for selection. 

The `config_file` provides configuration details, and the `penalty` parameter indicates whether the penalty should be applied to the fitness values used in selection.

Example:

```Python
a = Population("G1", config_file=f"inputs/params_g1.cfg", penalty=True)
```
### Evolving the population
Once the population has been created, you can evolve it using different strategies by calling the `evolve` method on your `Population` instance. The `evolve` method accepts a strategy name as an argument, such as `"GA"`, to apply the specified genetic algorithm.

Example:

```python
a.evolve("GA")
``` 
### Retrieving Population Information

After evolving the population, you can retrieve detailed information about it, including statistical data and the genes of the best-performing individual.

#### Get Population Statistics

The `get_population_statistics()` method provides summary statistics of the population, which can help you analyze the optimization progress and performance.

Example:

```python
population_stats = a.get_population_statistics()
print(population_stats)
```

#### Accessing the Best Chromosome's Genes
To examine the best solution found during the evolution process, access the genes attribute of the best_chromosome. This provides the gene values (solution variables) of the individual with the highest fitness.
```python
best_genes = a.best_chromosome.genes
print(best_genes)
```

All code examples provided in this README, including population creation, evolution, and retrieving population statistics, are available in the `main.ipynb` Jupyter Notebook. 

Simply open `main.ipynb` to explore and run the examples in a step-by-step format.

---

Author: [David Torres](https://github.com/proteusmax)
